## 计划、进度 3.6 
#### 关于DMFB
- [x] 完成对于DMFB的调研 （进行中）
- [x] 了解DMFB的routing-based和module-based的区别
- [x] 了解DMFB的基本算法，以及算法运行的模拟环境
- [ ] 了解DMFB算法要解决的问题，以及遇到的挑战
- [ ] 思考应用哪些SafeRL算法到DMFB领域

#### 关于SafeRL
- [x] 完成对于安全强化学习综述的调研
- [x] 了解安全强化学习的约束公式的基本介绍
- [x] 了解安全强化学习的基本算法
- [x] 思考SafeRL算法的多智能体形式
- [ ] 思考应用哪些安全强化学习算法到DMFB领域

目前已经调研了安全强化学习领域内的近期综述，了解最关键的一些约束形式、以及解决对应问题的算法。接下来主要任务就是结合DFMB的特点，思考算法对应的多智能体形式，如何运用到我们这个问题上来。以及如何针对我们的问题，对算法进行**针对性优化**。

目前看到的就是：PPO+Safety = SRL， CPO+VDN = MASRL，然后加水CTDE。 看到了一篇讲自动驾驶的，属于分层之后采用安全评论家+CBF做的。 其实就是情况越复杂，越需要考虑约束，就越需要采用安全强化学习的方法。
可以针对MADDPG之类的加入安全约束  
COMA（反事实多智能体策略梯度）？ROMA（面向角色的多智能体强化学习）？  

针对性优化的核心：找寻到我们的问题与他们的问题的最关键区别，提出对应的改进。比如电极失效、清洗液滴、混合液滴模式、

目前关于安全强化学习调研笔记都在safeRL这个文件夹下，运用了AI总结文章，并且对关键部分进行了仔细研读。

## 计划、进度 3.16
#### 关于安全强化学习
看安全强化学习有哪几种，安全评论家是怎么做到的
看自动驾驶领域都是怎么做到保证安全且到达目的地的。
看机器确认控制领域都是怎么保证不摔倒且完成动作的。
改动HIRO算法与安全评论家结合，上层算法控制子目标，子目标执行的时候，遇到安全障碍就自动避障。目标较为短期，子目标更可能完成任务。




#### 关于SafeRL
- [x] 完成对于安全强化学习综述的调研

目前已经调研了安全强化学习领域内的近期综述，了解最关键的一些约束形式、以及解决对应问题的算法。接下来主要任务就是结合DFMB的特点，思考算法对应的多智能体形式，如何运用到我们这个问题上来。以及如何针对我们的问题，对算法进行**针对性优化**。

目前看到的就是：PPO+Safety = SRL， CPO+VDN = MASRL，然后加水CTDE。 看到了一篇讲自动驾驶的，属于分层之后采用安全评论家+CBF做的。 其实就是情况越复杂，越需要考虑约束，就越需要采用安全强化学习的方法。
可以针对MADDPG之类的加入安全约束  
COMA（反事实多智能体策略梯度）？ROMA（面向角色的多智能体强化学习）？  

针对性优化的核心：找寻到我们的问题与他们的问题的最关键区别，提出对应的改进。比如电极失效、清洗液滴、混合液滴模式、

目前关于安全强化学习调研笔记都在safeRL这个文件夹下，运用了AI总结文章，并且对关键部分进行了仔细研读。


## 计划、进度 3.20
#### 关于代码
主要针对师姐那篇代码进行修改，加入安全强化学习层。

针对每一次的策略动作a，加入安全约束。
$a_{\text{safe}} = \text{SampleUntilSafe}(s, \pi_{\theta}, Q_C, w)$
$$
\text{SampleUntilSafe}(s, \pi, Q_C, w) \triangleq \begin{cases} 
a \sim \pi(s) & \text{若 } Q_C(s, a) \leq w \\
\text{递归调用} & \text{否则}
\end{cases}
$$

$Q_C(s, a) = \mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty} \gamma^t C(s_t, a_t, s_{t+1}) \mid s_0 = s, a_0 = a\right]$

\(\delta_i = C(s_i, a_i, s'_i) + \gamma Q_C(s'_i, a'_i) - Q_C(s_i, a_i)\)

定义安全约束函数$C(s, a, s')$，表示从状态$s$采取动作$a$到状态$s'$的危险程度。定义碰到障碍，危险程度+100. 使用一个CNN层+全连接来作为critic进行预测。

使用的是两层架构，第一层是原始的VDN网络，第二层是安全评论家网络，用于直接更改输出。

在跑代码的时候遇到了问题，一直爆显存，还在调试。具体效果还未知。



## 计划、进度 3.26

实现一个简单的演员评论家算法，用于测试安全约束的有效性。

原本测试10\*10的size、4个droplet、50\*10000个steps训练。 成功率是85%
原本测试20\*20的size、4个droplet、50\*10000个steps训练。 成功率是91%

改动之后，
使用10\*10的size、4个droplet、50\*10000个steps训练成功后，评估是97%。
使用20\*20的size、4个droplet、50\*10000个steps训练成功后，评估是99%。

![alt text](image-3.png)

具体实现，采用一个安全评论家作为硬约束，但是这个只采用了判断是否安全这个选项。当违反安全约束，那就运行下一个最可能的动作。这样的话也会影响原本的价值函数。这里也使用episilon greedy来进行探索。


但是其实把r改动了，（十倍）把碰撞之后惩罚加大，成功率也可以变高，具体我还在训，但是至少20\*20的size、4个droplet、50\*10000个steps训练成功后，评估是99%，第二次是100。