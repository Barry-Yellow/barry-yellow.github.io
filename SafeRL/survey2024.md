“A Survey of Constraint Formulations in Safe Reinforcement Learning”由Akifumi Wachi、Xun Shen、Yanan Sui撰写。文章主要围绕安全强化学习中基于约束准则的问题展开，详细介绍了多种常见的约束公式、相关算法及其理论关系，并对当前研究现状和未来方向进行了探讨。

### 引言
- **强化学习发展与应用**：强化学习在多领域取得显著进展，如游戏、机器人、数据中心冷却、金融、推荐系统、医疗等，近年来在大语言模型微调（RLHF）中也备受关注。
- **安全强化学习的重要性与研究现状**：应用强化学习于现实问题时，安全至关重要，安全强化学习因此受到广泛研究。它的优化标准可分为约束准则、最坏情况准则、风险敏感准则等，本文聚焦于约束准则。然而，该领域中安全约束的表示多样，其相互关系和理论联系缺乏分析，现有综述多关注方法而非公式，因此系统梳理相关文献意义重大。
- **本文贡献**：全面综述安全强化学习的约束公式，介绍各公式的代表性算法；通过定义可变换性、泛化性和保守近似三个理论概念，讨论不同约束公式之间的关系；将现有研究以约束公式为核心进行组织，为安全强化学习问题与合适算法搭建桥梁。

### 预备知识
- **约束马尔可夫决策过程（CMDP）**：本文将安全强化学习问题建模为CMDP，其形式为\(\mathcal{M} \cup \mathcal{C}:=\left<\mathcal{S}, \mathcal{A}, \mathcal{P}, H, r, \gamma_{r}, \rho\right> \cup \mathcal{C}\)，包含标准MDP的元素和表示安全约束的元组\(\mathcal{C}\)。
- **策略与价值函数**：策略\(\pi: S \to \Delta(A)\)将状态映射为动作分布，价值函数\(V_{r, h}^{\pi}(s):=\mathbb{E}_{\pi}\left[\sum_{h'=h}^{H} \gamma_{r}^{h'} r\left(s_{h'}, a_{h'}\right) | s_{h}=s\right]\)，基于初始状态分布\(\rho\)定义\(V_{r}^{\pi}(\rho):=\mathbb{E}_{s \sim \rho}\left[V_{r, 0}^{\pi}(s)\right]\)。
- **约束策略优化问题**：策略需在满足安全约束的可行策略空间\(\widehat{\Pi} \subseteq \Pi\)内，最优策略\(\pi^{*}:=\underset{\pi \in \hat{\Pi}}{arg max } V_{r}^{\pi}(\rho)\)。解决安全强化学习问题通常包括问题公式化和策略优化两个步骤，其中安全约束表示多样。

### 常见约束公式
- **期望累积安全约束**：用与奖励价值函数相似结构表示安全约束，定义安全价值函数\(V_{c, h}^{\pi}(s)\)和\(V_{c}^{\pi}(\rho)\)，问题为\(\max _{\pi} V_{r}^{\pi}(\rho) s.t. V_{c}^{\pi}(\rho) \leq \xi\) 。该公式亲和力高，基于它的拉格朗日方法等理论研究深入，CPO、RCPO等算法均基于此。
- **状态约束**：通过限制智能体访问不安全状态集合来表示安全约束，公式为\(\max _{\pi} V_{r}^{\pi}(\rho) s.t. \mathbb{E}_{\pi}\left[\sum_{h=0}^{H} \gamma_{c}^{h} \mathbb{I}\left(s_{h} \in S_{unsafe }\right)\right] \leq \xi\) ，常用于安全关键机器人任务。
- **联合机会约束**：在控制理论领域研究较多，形式为\(\max _{\pi} V_{r}^{\pi}(\rho) s.t. \mathbb{P}_{\pi}\left[\bigvee_{h=0}^{H} s_{h} \in S_{unsafe }\right] \leq \xi\) ，直接求解困难，常采用近似或假设方法。
- **时变阈值的期望瞬时安全约束**：要求智能体在每个时间步满足安全约束，公式为\(\max _{\pi} V_{r}^{\pi}(\rho) s.t. \mathbb{E}_{\pi}\left[c\left(s_{h}, a_{h}\right)\right] \leq \xi_{h}, \forall h \in[H]\) 。
- **几乎必然累积安全约束**：基于更严格的安全概念，确保安全几乎必然发生，问题为\(\max _{\pi} V_{r}^{\pi}(\rho) s.t. \mathbb{P}_{\pi}\left[\sum_{h=0}^{H} \gamma_{c}^{h} c\left(s_{h}, a_{h}\right) \leq \xi\right]=1\) 。
- **时不变阈值的几乎必然瞬时安全约束**：通过瞬时约束确保智能体在学习阶段安全探索，公式为\(\max _{\pi} V_{r}^{\pi}(\rho) s.t. \mathbb{P}_{\pi}\left[c\left(s_{h}, a_{h}\right) \leq \xi\right]=1, \forall h \in[H]\) ，与控制理论中的控制障碍函数或李雅普诺夫函数相关。
- **时变阈值的几乎必然瞬时安全约束**：类似上述公式，但安全阈值随时间变化，公式为\(\max V_{r}^{\pi}(\rho)\) S.t. \(\mathbb{P}_{\pi}[c(s_{h}, a_{h}) ≤\xi_{h}]=1\) , \(\forall h \in[H]\) ，应用范围更广，与问题5存在理论联系。
- **其他约束公式**：如用回报方差定义安全约束，在金融领域有用；去除遍历性假设并要求智能体返回初始安全状态；用形式语言或自然语言表示安全约束，可利用人类知识。

### 常见安全强化学习约束公式的理论关系
- **定义**：可变换性指两个CMDP的可行策略空间相同；泛化性指存在一个问题可被其他多个问题变换得到；保守近似指一个问题的可行策略空间包含另一个问题的可行策略空间。
- **初步引理**：引理1表明累加安全约束和瞬时安全约束通过定义新变量\(\eta_{h}\)存在理论联系；引理2证明问题1可转化为问题4；引理3证明问题5可转化为问题7。
- **两个IoMG - SafeRL问题**：定理1指出问题4是问题1和2的IoMG - SafeRL问题；定理2表明\(\gamma_{c}=1\)时问题4是问题3的保守近似；定理3说明问题7是问题5和6的IoMG - SafeRL问题。

### 讨论
- **公式和算法选择**：约束公式分为基于\(\mathbb{E}_{\pi}\)和\(\mathbb{P}_{\pi}\)两类，前者关注安全平均性能，后者保证更高安全水平但可能降低奖励性能，应根据问题需求选择。策略优化时，可选用现有开源软件中的算法，基于问题4和7的算法因瞬时约束更易处理而有前景。此外，还需考虑智能体满足安全约束的时机，分为训练后收敛时和训练及收敛后两个类别，应根据具体问题选择。
- **在线强化学习与离线强化学习**：现有安全强化学习多为在线设置，可从头训练策略；离线强化学习从预收集数据训练策略，在安全强化学习中有优势，目前多基于问题1，未来有望扩展到其他问题设置。

### 结论
本文全面综述基于约束优化准则的安全强化学习，介绍常见约束公式、算法及理论关系，描述研究现状并展望未来方向，旨在促进对约束公式的系统理解，推动安全强化学习的基础和应用研究。 