## 量化知识
参数量化
KV缓存量化

按通道量化
按token量化

异常值处理

旋转矩阵量化：将权重矩阵与旋转矩阵相乘以减少异常值并提高可量化性。
如何构造这个矩阵，为什么构造这个矩阵，如何还原这个量化过程

总结
按通道量化：
主要用于模型权重和 KV 缓存的 Key 张量，解决通道间分布差异问题。
按 Token 量化：
主要用于 KV 缓存的 Value 张量和输入激活值，适应动态序列特性。
KV 缓存的混合应用：
KIVI 方法中，Key 按通道量化，Value 按 Token 量化，两者协同优化显存与精度。



# VideoLLMs 中 KV 缓存分布特性的深度解析（结合模型结构）

## 一、Key 缓存的异常通道特性：视觉特征的 “高频波动区”
### 1.  异常通道的本质：视觉特征的特异性编码
#### 模型结构关联：
VideoLLMs（如 LLaVA-OV-7B）的视觉编码器（如 ViT）将视频帧分割为图像块（Patch），每个 Patch 生成一个 Token，每个 Token 包含多个通道（如 128 通道），每个通道对应一种视觉特征（如边缘、纹理、运动向量）。

异常通道：对应图像中高频变化区域（如物体边缘、运动轨迹）的特征通道，其幅值范围（Range）显著大于其他通道。例如，某通道编码 “物体轮廓”，在不同帧中因物体运动导致幅值剧烈变化（图 1a/c 显示 Key 缓存中部分通道的幅值波动远超均值）。

#### 量化挑战：
低比特量化（如 1 位）仅能表示两种状态（如 ±scale），而异常通道的幅值范围大（如 [-10, 10]），量化后会因值域截断导致严重信息丢失（如将 10 和 5 均量化为 1，误差达 5），进而影响注意力分数计算，导致模型推理错误。

### 2.  分布特性的数学表达
#### 通道级统计量：
设 Key 缓存矩阵 \(K \in \mathbb{R}^{T \times D}\)（T 为 Token 数，D 为通道数），第 \(d\) 通道的 Range 为：
$$\text{Range}(d) = \max(K[:, d]) - \min(K[:, d])$$
异常通道满足 \(\text{Range}(d) > \tau \cdot \text{Mean}(\text{Range})\)（τ 为阈值，实验中通过 TopK 筛选前 50% 通道）。

#### 模型结构影响：
视觉编码器的线性层（如 \(W_k \in \mathbb{R}^{d \times d}\)）将原始像素映射到高维空间，部分通道被强化以捕捉关键视觉变化，导致异常通道的存在（与文本模型中均匀分布的 Key 通道不同）。

## 二、Value 缓存的 Token 间差异：视觉特征的 “空间一致性”
### 1.  Token 间差异主导的原因：视觉特征的空间局部性
#### 模型结构关联：
VideoLLMs 的 Value 缓存存储视觉 Token 的语义值，用于生成注意力输出。同一通道（如通道 d）在不同 Token（不同 Patch）中对应图像中相同位置的特征（如 Patch (1,1) 和 Patch (2,1) 的通道 d 均编码图像左上方的纹理），因此：
- 通道内 Token 幅值一致：同一通道的 Token 在相邻 Patch 中幅值相近（图 1b/d 显示 Value 缓存通道内波动小）。
- Token 间差异显著：不同 Token（如前景物体 vs 背景）的同一通道幅值可能差异大（如物体边缘 Token 的通道 d 幅值为 5，背景 Token 为 0.5）。

#### 与 LLMs 的本质区别：
LLMs 的 Value 缓存中，Token 对应词语（如 “猫” vs “狗”），同一通道需编码不同语义，导致通道间差异大，适合按 Token 量化（Per-Token）；而 VideoLLMs 的 Value 通道对应空间位置特征，适合按通道量化（Per-Channel，统一缩放因子）。

### 2.  按通道量化的数学优势
#### 量化公式对比：
- Per-Token 量化（LLMs 常用）：每个 Token 独立计算缩放因子 \(s_t = \frac{\max(V_t) - \min(V_t)}{2^n - 1}\)，需存储 T 个缩放因子。
- Per-Channel 量化（VideoLLMs 适用）：每个通道统一缩放因子 \(s_d = \frac{\max(V[:, d]) - \min(V[:, d])}{2^n - 1}\)，仅需存储 D 个缩放因子，且更准确（通道内 Token 值域一致，图 1d/h 显示 Value 通道内 Range 小）。

#### 模型推理优化：
VideoLLMs 的注意力计算中，Value 与注意力权重的矩阵乘法可按通道向量化计算：
$$A[:, d] = \sum_{t=1}^T \text{AttnScore}(t) \cdot Q(V[t, d])$$
按通道量化后，同一通道内的所有 Token 共享缩放因子，计算效率提升 D 倍（对比 Per-Token 的 T 倍）。

## 三、模型结构对分布特性的影响：以 LLaVA-OV-7B 为例
### 1.  视觉编码器（ViT）的特征输出
- Patch 分割：将视频帧分割为 16x16 的 Patch，每个 Patch 生成 196 个 Token（如 ViT-Large），每个 Token 包含 128 通道。
- 通道功能分化：
    - 通道 0-32：编码颜色信息，幅值范围小（稳定）。
    - 通道 96-128：编码运动向量，幅值范围大（异常通道）。

### 2.  注意力头的通道选择
- 头 0-3：关注物体边缘（对应异常通道），Key 缓存中这些通道的 Range 显著高于其他头。
- 头 4-7：关注背景纹理（对应正常通道），Range 小，适合 1 位量化 + FFT 处理。

### 3.  实验验证：Per-Channel vs Per-Token
#### 数据对比（表 1）：
LLaVA-OV-7B 在 VideoDC 任务中，Value 缓存按通道量化（V-C）得分为 3.03，按 Token 量化（V-T）得 3.00，证明 Per-Channel 更适合视频场景。

#### 原因：
Per-Token 量化对同一通道内的相似 Token 重复计算缩放因子，浪费资源且引入误差；Per-Channel 利用空间一致性，减少冗余并提高精度。

## 四、总结：分布特性的模型依赖性与量化策略
| 缓存类型 | 分布特性 | 模型结构原因 | 量化策略 | 优势 |
| --- | --- | --- | --- | --- |
| Key 缓存 | 部分通道幅值异常大 / 波动剧烈 | 视觉编码器强化关键特征（如边缘、运动），导致通道分化 | 混合精度（异常通道 2 位 + 正常通道 1 位 + FFT） | 保留关键特征，降低频域误差 |
| Value 缓存 | Token 间幅值差异大，通道内一致性高 | 视觉 Token 对应空间位置，同一通道编码相同位置特征 | Per-Channel 1.58 位 + 关键 Token 保护 | 利用空间冗余，减少计算量并保留语义信息 |

### 关键结论：
VideoLLMs 的 KV 缓存分布特性由视觉编码器的空间特征编码方式决定：
- Key 缓存的异常通道是视觉特征选择性强化的结果，需混合精度量化 + 频域处理；
- Value 缓存的通道内一致性是空间局部性的产物，需按通道量化 + 语义保护。

这种模型结构依赖性正是 VidKV 区别于 LLMs 量化方法（如 KIVI）的核心，通过定制化策略实现了 1.x 位量化的突破性效果。