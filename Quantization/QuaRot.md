# QuaRot方法详解：量化对象与量化方法

QuaRot是一种端到端量化方案，通过旋转操作消除隐藏状态中的异常值，实现对大语言模型（LLM）的权重、激活值和KV缓存的高效低比特量化。以下从**量化对象**和**量化方法**两方面详细解析：


## 一、量化对象
QuaRot实现了对LLM全组件的4位量化，包括：

### 1. 权重（Weights）
- 所有线性层权重（如FFN的\(W_{\text{up}}, W_{\text{gate}}, W_{\text{down}}\)，注意力的\(W_q, W_k, W_v, W_{\text{out}}\)）。  
- 通过旋转操作降低权重矩阵的“不一致性”（incoherence），使元素值分布更均匀，避免极端异常值。

### 2. 激活值（Activations）
- 隐藏状态（Residuals，如FFN输入输出、注意力模块中间激活值）。  
- Feed-Forward网络（FFN）的中间激活值，如\(W_{\text{up}}\)和\(W_{\text{gate}}\)的输出。  

### 3. KV缓存（KV Cache）
- 注意力模块的键（Keys）和值（Values）缓存，用于生成阶段的快速访问。  
- 通过旋转键和值，消除缓存中的异常值，实现低比特量化。


## 二、量化方法
QuaRot通过**计算不变性**和**Hadamard变换**，在不改变模型输出的前提下，将旋转操作融入权重和激活值处理，分两阶段实现量化：

### 阶段1：权重修改与旋转融合（离线处理）
核心思想：利用正交矩阵（如Hadamard矩阵）旋转权重和激活值，消除异常值，同时保持模型计算不变性。  

#### 1. 计算不变性原理
- 若在权重矩阵\(W\)左侧乘以正交矩阵\(Q\)，并在输出端乘以\(Q^\top\)，则整体计算等价于原矩阵\(W\)，即：  
  $$
  Y = (X Q^\top) (Q W) = X W
  $$  
- 结合RMSNorm的归一化性质（旋转不改变范数），确保旋转后的激活值不影响后续计算。

#### 2. FFN模块处理
- **输入旋转**：对FFN的输入隐藏状态\(X\)应用Hadamard矩阵\(Q\)，得到\(XQ\)。  
- **权重融合**：将\(Q^\top\)融入输入权重矩阵（如\(W_{\text{up}}, W_{\text{gate}}\)），输出权重矩阵（如\(W_{\text{down}}\)）融入\(Q\)，确保：  
  $$
  W_{\text{up}} \leftarrow Q^\top W_{\text{up}}, \quad W_{\text{down}} \leftarrow W_{\text{down}} Q
  $$  
- **中间激活值旋转**：在FFN的非线性激活后，插入在线Hadamard变换，消除输出激活值的异常值（如图1右所示）。

#### 3. 注意力模块处理
- **键/值旋转**：对注意力的键（K）和值（V）投影矩阵应用头级Hadamard变换，如：  
  $$
  W_v \leftarrow W_v (I \otimes H_{\text{head}}), \quad W_{\text{out}} \leftarrow (I \otimes H_{\text{head}}) W_{\text{out}}
  $$  
  其中\(H_{\text{head}}\)是头维度的Hadamard矩阵，确保键/值在投影后分布均匀。  
- **查询/键在线旋转**：结合位置编码（如RoPE），对查询（Q）和键（K）应用在线Hadamard变换，消除缓存中的异常值，便于后续量化。

#### 4. KV缓存处理
- 对缓存的键和值进行分组（如组大小128）非对称量化，利用旋转后的均匀分布减少量化误差。

### 阶段2：在线量化操作（推理阶段）
#### 1. 权重量化
- **方法**：默认使用GPTQ（分组量化），也可使用简单舍入（RTN）。  
- **格式**：权重存储为4位整数（INT4），对称量化（每列尺度）或非对称量化（分组尺度）。  

#### 2. 激活值量化
- **输入激活**：对FFN和注意力的输入隐藏状态，采用对称每token量化（每行一个尺度），尺度为行最大绝对值除以7（INT4最大值）。  
- **中间激活**：FFN非线性激活后，通过在线Hadamard变换（FP32精度）处理，再量化为INT4。  

#### 3. 计算流程
- **矩阵乘法**：INT4权重与INT4激活值通过TensorCore计算，结果累加为INT32，再转换为FP16进行后续操作（如RMSNorm、非线性激活）。  
- **注意力计算**：查询保持FP16，键/值从INT4缓存中反量化后，用FP16计算点积和Softmax，减少内存访问开销。


## 三、核心创新点
1. **异常值消除**：通过Hadamard变换旋转隐藏状态，使激活值分布均匀（无异常值），避免传统方法保留高精度异常值的开销（如图1对比）。  
2. **端到端量化**：首次实现权重、激活、KV缓存全4位量化，无需额外校准数据（6/8位时完全无损）。  
3. **计算效率**：融合旋转操作到权重矩阵，仅增加少量在线Hadamard变换（如每个Transformer层\(1.5\)次变换），结合高效CUDA内核（如CUTLASS）提升推理速度。


## 总结
QuaRot通过**旋转+量化**的结合，解决了激活值和KV缓存的异常值难题，实现了LLM的高效低比特推理。量化对象覆盖模型全组件，方法上利用计算不变性融合正交变换，在线阶段通过对称/非对称量化和高效核优化，在精度、速度和内存上取得平衡，尤其在4位量化下保持99%的零-shot性能，为LLM部署提供了实用方案。